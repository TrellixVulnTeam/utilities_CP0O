#!/usr/bin/env bash

# kb_python main_build
# 
# This wrapper script is auto-generated by viash 0.5.11 and is thus a derivative
# work thereof. This software comes with ABSOLUTELY NO WARRANTY from Data
# Intuitive.
# 
# The component may contain files which fall under a different license. The
# authors of this component should specify the license in the header of such
# files, or include a separate license file detailing the licenses of all included
# files.

set -e

if [ -z "$VIASH_TEMP" ]; then
  VIASH_TEMP=/tmp
fi

# define helper functions
# ViashQuote: put quotes around non flag values
# $1     : unquoted string
# return : possibly quoted string
# examples:
#   ViashQuote --foo      # returns --foo
#   ViashQuote bar        # returns 'bar'
#   Viashquote --foo=bar  # returns --foo='bar'
function ViashQuote {
  if [[ "$1" =~ ^-+[a-zA-Z0-9_\-]+=.+$ ]]; then
    echo "$1" | sed "s#=\(.*\)#='\1'#"
  elif [[ "$1" =~ ^-+[a-zA-Z0-9_\-]+$ ]]; then
    echo "$1"
  else
    echo "'$1'"
  fi
}
# ViashRemoveFlags: Remove leading flag
# $1     : string with a possible leading flag
# return : string without possible leading flag
# examples:
#   ViashRemoveFlags --foo=bar  # returns bar
function ViashRemoveFlags {
  echo "$1" | sed 's/^--*[a-zA-Z0-9_\-]*=//'
}
# ViashSourceDir: return the path of a bash file, following symlinks
# usage   : ViashSourceDir ${BASH_SOURCE[0]}
# $1      : Should always be set to ${BASH_SOURCE[0]}
# returns : The absolute path of the bash file
function ViashSourceDir {
  SOURCE="$1"
  while [ -h "$SOURCE" ]; do
    DIR="$( cd -P "$( dirname "$SOURCE" )" >/dev/null 2>&1 && pwd )"
    SOURCE="$(readlink "$SOURCE")"
    [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE"
  done
  cd -P "$( dirname "$SOURCE" )" >/dev/null 2>&1 && pwd
}
# see https://en.wikipedia.org/wiki/Syslog#Severity_level
VIASH_LOGCODE_EMERGENCY=0
VIASH_LOGCODE_ALERT=1
VIASH_LOGCODE_CRITICAL=2
VIASH_LOGCODE_ERROR=3
VIASH_LOGCODE_WARNING=4
VIASH_LOGCODE_NOTICE=5
VIASH_LOGCODE_INFO=6
VIASH_LOGCODE_DEBUG=7
VIASH_VERBOSITY=$VIASH_LOGCODE_NOTICE

# ViashLog: Log events depending on the verbosity level
# usage: ViashLog 1 alert Oh no something went wrong!
# $1: required verbosity level
# $2: display tag
# $3+: messages to display
# stdout: Your input, prepended by '[$2] '.
function ViashLog {
  local required_level="$1"
  local display_tag="$2"
  shift 2
  if [ $VIASH_VERBOSITY -ge $required_level ]; then
    echo "[$display_tag]" "$@"
  fi
}

# ViashEmergency: log events when the system is unstable
# usage: ViashEmergency Oh no something went wrong.
# stdout: Your input, prepended by '[emergency] '.
function ViashEmergency {
  ViashLog $VIASH_LOGCODE_EMERGENCY emergency $@
}

# ViashAlert: log events when actions must be taken immediately (e.g. corrupted system database)
# usage: ViashAlert Oh no something went wrong.
# stdout: Your input, prepended by '[alert] '.
function ViashAlert {
  ViashLog $VIASH_LOGCODE_ALERT alert $@
}

# ViashCritical: log events when a critical condition occurs
# usage: ViashCritical Oh no something went wrong.
# stdout: Your input, prepended by '[critical] '.
function ViashCritical {
  ViashLog $VIASH_LOGCODE_CRITICAL critical $@
}

# ViashError: log events when an error condition occurs
# usage: ViashError Oh no something went wrong.
# stdout: Your input, prepended by '[error] '.
function ViashError {
  ViashLog $VIASH_LOGCODE_ERROR error $@
}

# ViashWarning: log potentially abnormal events
# usage: ViashWarning Something may have gone wrong.
# stdout: Your input, prepended by '[warning] '.
function ViashWarning {
  ViashLog $VIASH_LOGCODE_WARNING warning $@
}

# ViashNotice: log significant but normal events
# usage: ViashNotice This just happened.
# stdout: Your input, prepended by '[notice] '.
function ViashNotice {
  ViashLog $VIASH_LOGCODE_NOTICE notice $@
}

# ViashInfo: log normal events
# usage: ViashInfo This just happened.
# stdout: Your input, prepended by '[info] '.
function ViashInfo {
  ViashLog $VIASH_LOGCODE_INFO info $@
}

# ViashDebug: log all events, for debugging purposes
# usage: ViashDebug This just happened.
# stdout: Your input, prepended by '[debug] '.
function ViashDebug {
  ViashLog $VIASH_LOGCODE_DEBUG debug $@
}

# find source folder of this component
VIASH_META_RESOURCES_DIR=`ViashSourceDir ${BASH_SOURCE[0]}`

# backwards compatibility
VIASH_RESOURCES_DIR="$VIASH_META_RESOURCES_DIR"

# define meta fields
VIASH_META_FUNCTIONALITY_NAME="kb_python"


# ViashHelp: Display helpful explanation about this executable
function ViashHelp {
  echo "kb_python main_build"
  echo ""
  echo "Uknown"
  echo ""
  echo "Usage:"
  echo "python script.py \\"
  echo "  --input /path/to/bcl \\"
  echo "  --sample_sheet sample_sheet.csv \\"
  echo "  --output fastqs"
  echo ""
  echo "Options:"
  echo "    --input"
  echo "        type: file, required parameter"
  echo "        example: /path/to/bcl"
  echo "        Path to the untarred BCL files."
  echo ""
  echo "    --sample_sheet"
  echo "        type: file"
  echo "        example: SampleSheet.csv"
  echo "        The path to the sample sheet."
  echo ""
  echo "    --output"
  echo "        type: file, required parameter, output"
  echo "        example: /path/to/output"
  echo "        The folder to store the demux results"
  echo ""
  echo "    --star_structure"
  echo "        type: boolean_true"
  echo "        Group the fastq files into folders based on sample name"
  echo ""
  echo "    --skip_undetermined"
  echo "        type: boolean_true"
  echo "        Don't upload the Undetermined files (can save time)"
}
######## Helper functions for setting up Docker images for viash ########
# expects: ViashDockerBuild

# ViashDockerInstallationCheck: check whether Docker is installed correctly
#
# examples:
#   ViashDockerInstallationCheck
function ViashDockerInstallationCheck {
  ViashDebug "Checking whether Docker is installed"
  if [ ! command -v docker &> /dev/null ]; then
    ViashCritical "Docker doesn't seem to be installed. See 'https://docs.docker.com/get-docker/' for instructions."
    exit 1
  fi

  ViashDebug "Checking whether the Docker daemon is running"
  save=$-; set +e
  docker_version=$(docker version --format '{{.Client.APIVersion}}' 2> /dev/null)
  out=$?
  [[ $save =~ e ]] && set -e
  if [ $out -ne 0 ]; then
    ViashCritical "Docker daemon does not seem to be running. Try one of the following:"
    ViashCritical "- Try running 'dockerd' in the command line"
    ViashCritical "- See https://docs.docker.com/config/daemon/"
    exit 1
  fi
}

# ViashDockerRemoteTagCheck: check whether a Docker image is available 
# on a remote. Assumes `docker login` has been performed, if relevant.
#
# $1                  : image identifier with format `[registry/]image[:tag]`
# exit code $?        : whether or not the image was found
# examples:
#   ViashDockerRemoteTagCheck python:latest
#   echo $?                                     # returns '0'
#   ViashDockerRemoteTagCheck sdaizudceahifu
#   echo $?                                     # returns '1'
function ViashDockerRemoteTagCheck {
  docker manifest inspect $1 > /dev/null 2> /dev/null
}

# ViashDockerLocalTagCheck: check whether a Docker image is available locally
#
# $1                  : image identifier with format `[registry/]image[:tag]`
# exit code $?        : whether or not the image was found
# examples:
#   docker pull python:latest
#   ViashDockerLocalTagCheck python:latest
#   echo $?                                     # returns '0'
#   ViashDockerLocalTagCheck sdaizudceahifu
#   echo $?                                     # returns '1'
function ViashDockerLocalTagCheck {
  [ -n "$(docker images -q $1)" ]
}

# ViashDockerPull: pull a Docker image
#
# $1                  : image identifier with format `[registry/]image[:tag]`
# exit code $?        : whether or not the image was found
# examples:
#   ViashDockerPull python:latest
#   echo $?                                     # returns '0'
#   ViashDockerPull sdaizudceahifu
#   echo $?                                     # returns '1'
function ViashDockerPull {
  ViashNotice "Checking if Docker image is available at '$1'"
  if [ $VIASH_VERBOSITY -ge $VIASH_LOGCODE_INFO ]; then
    docker pull $1 && return 0 || return 1
  else
    save=$-; set +e
    docker pull $1 2> /dev/null > /dev/null
    out=$?
    [[ $save =~ e ]] && set -e
    if [ $out -ne 0 ]; then
      ViashWarning "Could not pull from '$1'. Docker image doesn't exist or is not accessible."
    fi
    return $out
  fi
}

# ViashDockerPush: push a Docker image
#
# $1                  : image identifier with format `[registry/]image[:tag]`
# exit code $?        : whether or not the image was found
# examples:
#   ViashDockerPush python:latest
#   echo $?                                     # returns '0'
#   ViashDockerPush sdaizudceahifu
#   echo $?                                     # returns '1'
function ViashDockerPush {
  ViashNotice "Pushing image to '$1'"
  save=$-; set +e
  if [ $VIASH_VERBOSITY -ge $VIASH_LOGCODE_INFO ]; then
    docker push $1
    out=$?
  else
    docker push $1 2> /dev/null > /dev/null
    out=$?
  fi
  [[ $save =~ e ]] && set -e
  if [ $out -eq 0 ]; then
    ViashNotice "Container '$VSHD_ID' push succeeded."
  else
    ViashError "Container '$VSHD_ID' push errored. You might not be logged in or have the necessary permissions."
  fi
  return $out
}

# ViashDockerPullElseBuild: pull a Docker image, else build it
#
# $1                  : image identifier with format `[registry/]image[:tag]`
# ViashDockerBuild    : a Bash function which builds a docker image, takes image identifier as argument.
# examples:
#   ViashDockerPullElseBuild mynewcomponent
function ViashDockerPullElseBuild {
  save=$-; set +e
  ViashDockerPull $1
  out=$?
  [[ $save =~ e ]] && set -e
  if [ $out -ne 0 ]; then
    ViashDockerBuild $@
  fi
}

# ViashDockerSetup: create a Docker image, according to specified docker setup strategy
#
# $1                  : image identifier with format `[registry/]image[:tag]`
# $2                  : docker setup strategy, see DockerSetupStrategy.scala
# ViashDockerBuild    : a Bash function which builds a docker image, takes image identifier as argument.
# examples:
#   ViashDockerSetup mynewcomponent alwaysbuild
function ViashDockerSetup {
  VSHD_ID="$1"
  VSHD_STRAT="$2"
  if [ "$VSHD_STRAT" == "alwaysbuild" -o "$VSHD_STRAT" == "build" -o "$VSHD_STRAT" == "b" ]; then
    ViashDockerBuild $VSHD_ID --no-cache
  elif [ "$VSHD_STRAT" == "alwayspull" -o "$VSHD_STRAT" == "pull" -o "$VSHD_STRAT" == "p" ]; then
    ViashDockerPull $VSHD_ID
  elif [ "$VSHD_STRAT" == "alwayspullelsebuild" -o "$VSHD_STRAT" == "pullelsebuild" ]; then
    ViashDockerPullElseBuild $VSHD_ID --no-cache
  elif [ "$VSHD_STRAT" == "alwayspullelsecachedbuild" -o "$VSHD_STRAT" == "pullelsecachedbuild" ]; then
    ViashDockerPullElseBuild $VSHD_ID
  elif [ "$VSHD_STRAT" == "alwayscachedbuild" -o "$VSHD_STRAT" == "cachedbuild" -o "$VSHD_STRAT" == "cb" ]; then
    ViashDockerBuild $VSHD_ID
  elif [[ "$VSHD_STRAT" =~ ^ifneedbe ]]; then
    save=$-; set +e
    ViashDockerLocalTagCheck $VSHD_ID
    outCheck=$?
    [[ $save =~ e ]] && set -e
    if [ $outCheck -eq 0 ]; then
      ViashInfo "Image $VSHD_ID already exists"
    elif [ "$VSHD_STRAT" == "ifneedbebuild" ]; then
      ViashDockerBuild $VSHD_ID --no-cache
    elif [ "$VSHD_STRAT" == "ifneedbecachedbuild" ]; then
      ViashDockerBuild $VSHD_ID
    elif [ "$VSHD_STRAT" == "ifneedbepull" ]; then
      ViashDockerPull $VSHD_ID
    elif [ "$VSHD_STRAT" == "ifneedbepullelsebuild" ]; then
      ViashDockerPullElseBuild $VSHD_ID --no-cache
    elif [ "$VSHD_STRAT" == "ifneedbepullelsecachedbuild" ]; then
      ViashDockerPullElseBuild $VSHD_ID
    else
      ViashError "Unrecognised Docker strategy: $VSHD_STRAT"
      exit 1
    fi
  elif [ "$VSHD_STRAT" == "push" -o "$VSHD_STRAT" == "forcepush" -o "$VSHD_STRAT" == "alwayspush" ]; then
    ViashDockerPush "$VSHD_ID"
  elif [ "$VSHD_STRAT" == "pushifnotpresent" -o "$VSHD_STRAT" == "gentlepush" -o "$VSHD_STRAT" == "maybepush" ]; then
    save=$-; set +e
    ViashDockerRemoteTagCheck $VSHD_ID
    outCheck=$?
    [[ $save =~ e ]] && set -e
    if [ $outCheck -eq 0 ]; then
      ViashNotice "Container '$VSHD_ID' exists, doing nothing."
    else
      ViashNotice "Container '$VSHD_ID' does not yet exist."
      ViashDockerPush "$VSHD_ID"
    fi
  elif [ "$VSHD_STRAT" == "donothing" -o "$VSHD_STRAT" == "meh" ]; then
    ViashNotice "Skipping setup."
  else
    ViashError "Unrecognised Docker strategy: $VSHD_STRAT"
    exit 1
  fi
}


######## End of helper functions for setting up Docker images for viash ########

# ViashDockerFile: print the dockerfile to stdout
# return : dockerfile required to run this component
# examples:
#   ViashDockerFile
function ViashDockerfile {
  cat << 'VIASHDOCKER'
FROM ghcr.io/data-intuitive/cellranger:6.1

RUN :
LABEL org.opencontainers.image.description="Companion container for running component rna_velocity kb_python"
LABEL org.opencontainers.image.source="https://github.com/czbiohub/utilities"
VIASHDOCKER
}

# ViashDockerBuild: build a docker container
# $1              : image identifier with format `[registry/]image[:tag]`
# exit code $?    : whether or not the image was built
function ViashDockerBuild {
  # create temporary directory to store dockerfile & optional resources in
  tmpdir=$(mktemp -d "$VIASH_TEMP/viashsetupdocker-kb_python-XXXXXX")
  function clean_up {
    rm -rf "$tmpdir"
  }
  trap clean_up EXIT

  # store dockerfile and resources
  ViashDockerfile > $tmpdir/Dockerfile
  cp -r $VIASH_META_RESOURCES_DIR/* $tmpdir

  # Build the container
  ViashNotice "Building container '$1' with Dockerfile"
  ViashInfo "Running 'docker build -t $@ $tmpdir'"
  save=$-; set +e
  if [ $VIASH_VERBOSITY -ge $VIASH_LOGCODE_INFO ]; then
    docker build -t $@ $tmpdir
  else
    docker build -t $@ $tmpdir &> $tmpdir/docker_build.log
  fi
  out=$?
  [[ $save =~ e ]] && set -e
  if [ $out -ne 0 ]; then
    ViashError "Error occurred while building container '$1'"
    if [ $VIASH_VERBOSITY -lt $VIASH_LOGCODE_INFO ]; then
      ViashError "Transcript: --------------------------------"
      cat "$tmpdir/docker_build.log"
      ViashError "End of transcript --------------------------"
    fi
    exit 1
  fi
}
# ViashAbsolutePath: generate absolute path from relative path
# borrowed from https://stackoverflow.com/a/21951256
# $1     : relative filename
# return : absolute path
# examples:
#   ViashAbsolutePath some_file.txt   # returns /path/to/some_file.txt
#   ViashAbsolutePath /foo/bar/..     # returns /foo
function ViashAbsolutePath {
  local thePath
  if [[ ! "$1" =~ ^/ ]]; then
    thePath="$PWD/$1"
  else
    thePath="$1"
  fi
  echo "$thePath" | (
    IFS=/
    read -a parr
    declare -a outp
    for i in "${parr[@]}"; do
      case "$i" in
      ''|.) continue ;;
      ..)
        len=${#outp[@]}
        if ((len==0)); then
          continue
        else
          unset outp[$((len-1))]
        fi
        ;;
      *)
        len=${#outp[@]}
        outp[$len]="$i"
      ;;
      esac
    done
    echo /"${outp[*]}"
  )
}
# ViashAutodetectMount: auto configuring docker mounts from parameters
# $1                  : The parameter value
# returns             : New parameter
# $VIASH_EXTRA_MOUNTS : Added another parameter to be passed to docker
# examples:
#   ViashAutodetectMount /path/to/bar      # returns '/viash_automount/path/to/bar'
#   ViashAutodetectMountArg /path/to/bar   # returns '-v /path/to:/viash_automount/path/to'
function ViashAutodetectMount {
  abs_path=$(ViashAbsolutePath "$1")
  if [ -d "$abs_path" ]; then
    mount_source="$abs_path"
    base_name=""
  else
    mount_source=`dirname "$abs_path"`
    base_name=`basename "$abs_path"`
  fi
  mount_target="/viash_automount$mount_source"
  echo "$mount_target/$base_name"
}
function ViashAutodetectMountArg {
  abs_path=$(ViashAbsolutePath "$1")
  if [ -d "$abs_path" ]; then
    mount_source="$abs_path"
    base_name=""
  else
    mount_source=`dirname "$abs_path"`
    base_name=`basename "$abs_path"`
  fi
  mount_target="/viash_automount$mount_source"
  echo "-v \"$mount_source:$mount_target\""
}
# ViashExtractFlags: Retain leading flag
# $1     : string with a possible leading flag
# return : leading flag
# examples:
#   ViashExtractFlags --foo=bar  # returns --foo
function ViashExtractFlags {
  echo $1 | sed 's/=.*//'
}
# initialise variables
VIASH_EXTRA_MOUNTS=''

# initialise array
VIASH_POSITIONAL_ARGS=''
VIASH_MODE='run'

while [[ $# -gt 0 ]]; do
    case "$1" in
        -h|--help)
            ViashHelp
            exit
            ;;
        ---v|---verbose)
            let "VIASH_VERBOSITY=VIASH_VERBOSITY+1"
            shift 1
            ;;
        ---verbosity)
            VIASH_VERBOSITY="$2"
            shift 2
            ;;
        ---verbosity=*)
            VIASH_VERBOSITY="$(ViashRemoveFlags "$1")"
            shift 1
            ;;
        --version)
            echo "kb_python main_build"
            exit
            ;;
        --input)
            VIASH_PAR_INPUT="$2"
            shift 2
            ;;
        --input=*)
            VIASH_PAR_INPUT=$(ViashRemoveFlags "$1")
            shift 1
            ;;
        --sample_sheet)
            VIASH_PAR_SAMPLE_SHEET="$2"
            shift 2
            ;;
        --sample_sheet=*)
            VIASH_PAR_SAMPLE_SHEET=$(ViashRemoveFlags "$1")
            shift 1
            ;;
        --output)
            VIASH_PAR_OUTPUT="$2"
            shift 2
            ;;
        --output=*)
            VIASH_PAR_OUTPUT=$(ViashRemoveFlags "$1")
            shift 1
            ;;
        --star_structure)
            VIASH_PAR_STAR_STRUCTURE=true
            shift 1
            ;;
        --skip_undetermined)
            VIASH_PAR_SKIP_UNDETERMINED=true
            shift 1
            ;;
        ---setup)
            VIASH_MODE='docker_setup'
            VIASH_DOCKER_SETUP_STRATEGY="$2"
            shift 1
            ;;
        ---setup=*)
            VIASH_MODE='docker_setup'
            VIASH_DOCKER_SETUP_STRATEGY="$(ViashRemoveFlags "$1")"
            shift 2
            ;;
        ---dockerfile)
            ViashDockerfile
            exit 0
            ;;
        ---v|---volume)
            VIASH_EXTRA_MOUNTS="$VIASH_EXTRA_MOUNTS -v "$2""
            shift 2
            ;;
        ---volume=*)
            VIASH_EXTRA_MOUNTS="$VIASH_EXTRA_MOUNTS -v $(ViashRemoveFlags "$2")"
            shift 1
            ;;
        ---debug)
            VIASH_MODE='docker_debug'
            shift 1
            ;;
        *)  # positional arg or unknown option
            # since the positional args will be eval'd, can we always quote, instead of using ViashQuote
            VIASH_POSITIONAL_ARGS="$VIASH_POSITIONAL_ARGS '$1'"
            shift # past argument
            ;;
    esac
done

# parse positional parameters
eval set -- $VIASH_POSITIONAL_ARGS


ViashDockerInstallationCheck
if [ $VIASH_MODE == "docker_setup" ]; then
  ViashDockerSetup 'ghcr.io/czbiohub/utilities/rna_velocity_kb_python:main_build' "$VIASH_DOCKER_SETUP_STRATEGY"
  exit 0
fi
ViashDockerSetup 'ghcr.io/czbiohub/utilities/rna_velocity_kb_python:main_build' ifneedbepullelsecachedbuild
if [ $VIASH_MODE == "docker_debug" ]; then
  ViashNotice "+ docker run --entrypoint=bash -i --rm -v "$(pwd)":/pwd --workdir /pwd -t 'ghcr.io/czbiohub/utilities/rna_velocity_kb_python:main_build'"
  docker run --entrypoint=bash -i --rm -v "$(pwd)":/pwd --workdir /pwd -t 'ghcr.io/czbiohub/utilities/rna_velocity_kb_python:main_build'
  exit 0
fi




# check whether required parameters exist
if [ -z "$VIASH_PAR_INPUT" ]; then
  ViashError '--input' is a required argument. Use "--help" to get more information on the parameters.
  exit 1
fi
if [ -z "$VIASH_PAR_OUTPUT" ]; then
  ViashError '--output' is a required argument. Use "--help" to get more information on the parameters.
  exit 1
fi
if [ -z "$VIASH_PAR_STAR_STRUCTURE" ]; then
  VIASH_PAR_STAR_STRUCTURE="false"
fi
if [ -z "$VIASH_PAR_SKIP_UNDETERMINED" ]; then
  VIASH_PAR_SKIP_UNDETERMINED="false"
fi


# detect volumes from file arguments
if [ ! -z "$VIASH_PAR_INPUT" ]; then
  VIASH_EXTRA_MOUNTS="$VIASH_EXTRA_MOUNTS $(ViashAutodetectMountArg "$VIASH_PAR_INPUT")"
  VIASH_PAR_INPUT=$(ViashAutodetectMount "$VIASH_PAR_INPUT")
fi
if [ ! -z "$VIASH_PAR_SAMPLE_SHEET" ]; then
  VIASH_EXTRA_MOUNTS="$VIASH_EXTRA_MOUNTS $(ViashAutodetectMountArg "$VIASH_PAR_SAMPLE_SHEET")"
  VIASH_PAR_SAMPLE_SHEET=$(ViashAutodetectMount "$VIASH_PAR_SAMPLE_SHEET")
fi
if [ ! -z "$VIASH_PAR_OUTPUT" ]; then
  VIASH_EXTRA_MOUNTS="$VIASH_EXTRA_MOUNTS $(ViashAutodetectMountArg "$VIASH_PAR_OUTPUT")"
  VIASH_PAR_OUTPUT=$(ViashAutodetectMount "$VIASH_PAR_OUTPUT")
fi

# Always mount the resource directory
VIASH_EXTRA_MOUNTS="$VIASH_EXTRA_MOUNTS $(ViashAutodetectMountArg "$VIASH_META_RESOURCES_DIR")"
VIASH_META_RESOURCES_DIR=$(ViashAutodetectMount "$VIASH_META_RESOURCES_DIR")

# Always mount the VIASH_TEMP directory
VIASH_EXTRA_MOUNTS="$VIASH_EXTRA_MOUNTS $(ViashAutodetectMountArg "$VIASH_TEMP")"
VIASH_TEMP=$(ViashAutodetectMount "$VIASH_TEMP")
# change file ownership
function ViashPerformChown {
  
  if [ ! -z "$VIASH_PAR_OUTPUT" ]; then
    eval docker run --entrypoint=chown -i --rm $VIASH_EXTRA_MOUNTS ghcr.io/czbiohub/utilities/rna_velocity_kb_python:main_build "$(id -u):$(id -g)" --silent --recursive "$VIASH_PAR_OUTPUT"
  fi
}
trap ViashPerformChown EXIT


cat << VIASHEOF | eval docker run --entrypoint=bash -i --rm $VIASH_EXTRA_MOUNTS ghcr.io/czbiohub/utilities/rna_velocity_kb_python:main_build
set -e
tempscript=\$(mktemp "$VIASH_TEMP/viash-run-kb_python-XXXXXX")
function clean_up {
  rm "\$tempscript"
}
function interrupt {
  echo -e "\nCTRL-C Pressed..."
  exit 1
}
trap clean_up EXIT
trap interrupt INT SIGINT
cat > "\$tempscript" << 'VIASHMAIN'
## VIASH START
# The following code has been auto-generated by Viash.
par = {
  'input': $( if [ ! -z ${VIASH_PAR_INPUT+x} ]; then echo "'${VIASH_PAR_INPUT//\'/\\\'}'"; else echo None; fi ),
  'sample_sheet': $( if [ ! -z ${VIASH_PAR_SAMPLE_SHEET+x} ]; then echo "'${VIASH_PAR_SAMPLE_SHEET//\'/\\\'}'"; else echo None; fi ),
  'output': $( if [ ! -z ${VIASH_PAR_OUTPUT+x} ]; then echo "'${VIASH_PAR_OUTPUT//\'/\\\'}'"; else echo None; fi ),
  'star_structure': $( if [ ! -z ${VIASH_PAR_STAR_STRUCTURE+x} ]; then echo "'${VIASH_PAR_STAR_STRUCTURE//\'/\\\'}'.lower() == 'true'"; else echo None; fi ),
  'skip_undetermined': $( if [ ! -z ${VIASH_PAR_SKIP_UNDETERMINED+x} ]; then echo "'${VIASH_PAR_SKIP_UNDETERMINED//\'/\\\'}'.lower() == 'true'"; else echo None; fi )
}
meta = {
  'functionality_name': '$VIASH_META_FUNCTIONALITY_NAME',
  'resources_dir': '$VIASH_META_RESOURCES_DIR',
  'temp_dir': '$VIASH_TEMP'
}

resources_dir = '$VIASH_META_RESOURCES_DIR'

## VIASH END
#!/usr/bin/env python
import argparse
import datetime
import os
import pathlib
import posixpath
import re
import subprocess
import tarfile
import time
import sys
import shutil
import glob

from collections import defaultdict

import utilities.log_util as ut_log
import utilities.s3_util as s3u

import boto3
from boto3.s3.transfer import TransferConfig


def get_default_requirements():
    return argparse.Namespace(vcpus=16, memory=64000, storage=500, ecr_image="velocyto")


def display_info(logger):
    """Displays kb, kallisto and bustools version + citation information, along
    with a brief description and examples.
    
    Keyword Argument:
    mainlogger - Logger of main function (type: logging.Logger)
    """
    info_command = ["kb", "info"]

    #     if ut_log.log_command(
    #         logger,
    #         info_command,
    #         stdout=subprocess.PIPE,
    #         stderr=subprocess.STDOUT,
    #         shell=True,
    #     ):
    #         logger.info("Failed to view kb_python package details")
    #     sys.exit(1)

    proc = subprocess.run(" ".join(info_command), **kwargs)

    if proc.returncode != 0:
        raise RuntimeError("\`info\` command failed")
        if proc.stdout and isinstance(proc.stdout, str):
            raise RuntimeError(proc.stdout)
        elif isinstance(proc.stdout, bytes):
            raise RuntimeError(proc.stdout.decode())

        return True
    else:
        return False


def display_technologies(logger):
    """Displays a list of supported technologies along with whether kb provides
    a whitelist for that technology and the FASTQ argument order for kb count.
    
    Keyword Argument:
    mainlogger - Logger of main function (type: logging.Logger)
    """
    technology_command = ["kb", "--list"]

    if ut_log.log_command(
        logger,
        technology_command,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        shell=True,
    ):
        logger.info(
            "Failed to view single-cell technology list compatible with the kb_python package"
        )
    sys.exit(1)


#     proc = subprocess.run(" ".join(technology_command), **kwargs)

#     if proc.returncode != 0:
#         raise RuntimeError("\`--list\` command failed")
#         if proc.stdout and isinstance(proc.stdout, str):
#             raise RuntimeError(proc.stdout)
#         elif isinstance(proc.stdout, bytes):
#             raise RuntimeError(proc.stdout.decode())

#         return True
#     else:
#         return False


def parse_ref(args, run_dir, logger):
    """Parser for the \`ref\` command. Build kallisto index files.
    
    Keyword Arguments:
    args -- Command-line arguments dictionary, as parsed by argparse (type: dict)
    run_dir -- Path on the EC2 instance under which jobs are run (type: pathlib.Path)
    logger -- Logger object that exposes the interface the code directly uses (type: logging.Logger)
    """
    # Build paths on the EC2 instance to store inputs and outputs of kallisto index building. Download kallisto index building files from the AWS S3 bucket to the EC2 instance, if not using the in-built kallisto indices
    kallisto_index_dir = run_dir / "kallisto_index"
    kallisto_index_inputs = kallisto_index_dir / "inputs"
    kallisto_index_outputs = kallisto_index_dir / "outputs"
    kallisto_index_inputs.mkdir(parents=True)
    kallisto_index_outputs.mkdir(parents=True)
    kb_ref_paths = dict()
    s3_kb_ref = dict()
    kb_ref_output_to_s3 = dict()

    if "-d" not in sys.argv:
        if "--workflow" in sys.argv and "kite" in sys.argv:
            kb_ref_paths["feature_path"] = kallisto_index_inputs / os.path.basename(
                args.feature
            )
            (
                s3_kb_ref["s3_feature_bucket"],
                s3_kb_ref["s3_feature_prefix"],
            ) = s3u.s3_bucket_and_key(args.feature)
            s3c.download_file(
                Bucket=s3_kb_ref["s3_feature_bucket"],
                Key=s3_kb_ref["s3_feature_prefix"],
                Filename=str(kb_ref_paths["feature_path"]),
            )

        for arg in ["fasta", "gtf"]:
            kb_ref_paths[arg + "_path"] = kallisto_index_inputs / os.path.basename(
                getattr(args, arg)
            )
            (
                s3_kb_ref["s3_" + arg + "_bucket"],
                s3_kb_ref["s3_" + arg + "_prefix"],
            ) = s3u.s3_bucket_and_key(getattr(args, arg))
            s3c.download_file(
                Bucket=s3_kb_ref["s3_" + arg + "_bucket"],
                Key=s3_kb_ref["s3_" + arg + "_prefix"],
                Filename=str(kb_ref_paths[arg + "_path"]),
            )

    for arg in ["-i", "-g", "-f1", "-f2", "-c1", "-c2", "--tmp"]:
        if arg in sys.argv:
            arg = arg[2:] if arg == "--tmp" else arg[1:]
            print(
                f"testing purpose - see if getattr(args, arg) output all argument names from '-i', '-g', '-f1', '-f2', '-c1', '-c2', '--tmp': {getattr(args, arg)}"
            )  # testing purpose
            if arg == "tmp":
                kb_ref_paths[arg + "_path"] = kallisto_index_dir / "alter_tmp"
                s3_kb_ref["s3_" + arg + "_bucket"] = s3u.s3_bucket_and_key(
                    getattr(args, arg)
                )[0]
                s3_kb_ref["s3_" + arg + "_prefix"] = posixpath.join(
                    s3u.s3_bucket_and_key(getattr(args, arg))[1], "alter_tmp"
                )
            else:
                kb_ref_paths[arg + "_path"] = kallisto_index_outputs / os.path.basename(
                    getattr(args, arg)
                )
                (
                    s3_kb_ref["s3_" + arg + "_bucket"],
                    s3_kb_ref["s3_" + arg + "_prefix"],
                ) = s3u.s3_bucket_and_key(getattr(args, arg))

    # Build the command of running \`kb ref\` to generate kallisto index files
    ref_input_boolean = ["--lamanno", "--overwrite", "--keep-tmp", "--verbose"]
    ref_input_upload_required = ["-i", "-g", "-f1", "-f2", "-c1", "-c2", "--tmp"]
    ref_input_left_args = ["-d", "-n", "-k", "--workflow"]

    kb_ref_command = ["kb", "ref"]
    for input in ["fasta", "gtf"]:
        if "-d" not in sys.argv:
            print(
                f"testing purpose: \`ref\` positional argument: {getattr(args, input)}"
            )  # testing purpose
            if "--workflow" in sys.argv and "kite" in sys.argv:
                print(
                    f"testing purpose: \`ref\` positional argument: {getattr(args, input)}"
                )  # testing purpose
                kb_ref_command += [str(kb_ref_paths["feature_path"])]
            kb_ref_command += [str(kb_ref_paths[input + "_path"])]
    for input in ref_input_boolean:
        if input in sys.argv:
            kb_ref_command += [input]
    for input in ref_input_upload_required:
        if input in sys.argv:
            kb_ref_command += [
                input,
                str(kb_ref_paths[input[2:] + "_path"])
                if input == "--tmp"
                else str(kb_ref_paths[input[1:] + "_path"]),
            ]
    for input in ref_input_left_args:
        if input in sys.argv:
            kb_ref_command += [input, getattr(args, input)]
    print(
        f"testing purpose - check if kb_ref_command is of correct format: {kb_ref_command}"
    )  # testing purpose

    # Run the command to generate kallisto index files, and upload the output files on the EC2 instance back to AWS S3
    kb_ref_failed = ut_log.log_command(
        logger,
        kb_ref_command,
        shell=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
    )
    kb_ref_t_config = TransferConfig(
        use_threads=False
    )  # testing purpose: comment this line if this runs into error. kallisto indices are not pretty big and don't necessarily need a transfer config
    if kb_ref_failed:
        raise RuntimeError("kallisto index building failed")
    else:
        kb_ref_upload_files = glob.glob(str(kallisto_index_outputs / "*"))
        print(
            f"testing purpose - check kallisto index files: {kb_ref_upload_files}"
        )  # testing purpose
        for file in kb_ref_upload_files:
            logger.info(
                f"Uploading {os.path.basename(file)} from the EC2 instance to AWS S3"
            )
            s3c.upload_file(
                Filename=file,
                Bucket=kb_ref_output_to_s3[file][0],
                Key=kb_ref_output_to_s3[file][1],
                Config=kb_ref_t_config,
            )
            time.sleep(30)
            print(
                f"testing purpose - see if kb_ref upload_file function intakes correct bucket and prefix names for kallisto index output files: {kb_ref_output_to_s3[file][0]}, {kb_ref_output_to_s3[file][1]}"
            )  # testing purpose
    return


def parse_count(args, run_dir, logger):
    """Parser for the \`count\` command. Data quantification with kallisto and bustools.
    
    Keyword Arguments:
    args -- Command-line arguments dictionary, as parsed by argparse (type: dict)
    run_dir -- Path on the EC2 instance under which jobs are run (type: pathlib.Path)
    logger -- Logger object that exposes the interface the code directly uses (type: logging.Logger)
    """
    # Build paths on the EC2 instance to store inputs and outputs of kb data quantification results.
    kb_count_dir = run_dir / "kb_count"
    kb_fastqs = kb_count_dir / "fastqs"
    kb_count_inputs = kb_count_dir / "inputs"
    kb_count_outputs = kb_count_dir / "outputs"
    kb_fastqs.mkdir(parents=True)
    kb_count_inputs.mkdir(parents=True)
    kb_count_outputs.mkdir(parents=True)
    kb_count_paths = dict()
    s3_kb_count = dict()

    for arg in ["--tmp", "-o", "-w", "-i", "-g", "-c1", "-c2"]:
        if arg in sys.argv:
            arg = arg[2:] if arg == "--tmp" else arg[1:]
            print(
                f"testing purpose - see if getattr(args, arg) returns the correct values for \`kb count\` inputs: {getattr(args, arg)}"
            )  # testing purpose
            if arg == "tmp":
                kb_count_paths[arg + "_path"] = kb_count_dir / "alter_tmp"
                s3_kb_count["s3_" + arg + "_prefix"] = posixpath.join(
                    s3u.s3_bucket_and_key(getattr(args, arg))[1], "alter_tmp"
                )
            elif arg == "o":
                kb_count_paths[arg + "_path"] = kb_count_outputs
                s3_kb_count["s3_" + arg + "_prefix"] = posixpath.join(
                    s3u.s3_bucket_and_key(getattr(args, arg))[1], "outputs"
                )
            else:
                kb_count_paths[arg + "_path"] = kb_count_inputs / os.path.basename(
                    getattr(args, arg)
                )
                (
                    s3_kb_count["s3_" + arg + "_bucket"],
                    s3_kb_count["s3_" + arg + "_prefix"],
                ) = s3u.s3_bucket_and_key(getattr(args, arg))
                s3c.download_file(
                    Bucket=s3_kb_count["s3_" + arg + "_bucket"],
                    Key=s3_kb_count["s3_" + arg + "_prefix"],
                    Filename=str(kb_count_paths[arg + "_path"]),
                )
            s3_kb_count["s3_" + arg + "_bucket"] = s3u.s3_bucket_and_key(
                getattr(args, arg)
            )[0]

    # Download fastq files from the AWS S3 bucket to the EC2 instance.
    (
        kb_count_paths["fastqs_path"],
        s3_kb_count["s3_fastqs_bucket"],
        s3_kb_count["s3_fastqs_prefix"],
    ) = (dict(), dict(), dict())
    kb_count_fastq_files_paths, s3_kb_count_fastqs_bucket, s3_kb_count_fastqs_prefix = (
        kb_count_paths["fastqs_path"],
        s3_kb_count["s3_fastqs_bucket"],
        s3_kb_count["s3_fastqs_prefix"],
    )

    s3_fastq_folder_bucket, s3_fastq_folder_prefix = s3u.s3_bucket_and_key(
        args.fastq_folder
    )
    s3_fastq_files_prefix = list(
        s3u.get_files(bucket=s3_fastq_folder_bucket, prefix=s3_fastq_folder_prefix)
    )[1:]
    print(
        "testing purpose - see if all fastq files prefix are extracted: {s3_fastq_files_prefix}"
    )  # testing purpose
    fastq_format = re.compile("([^/]+)_R\\d(?:_\\d+)?.fastq.gz\$")

    for fastq_prefix in s3_fastq_files_prefix:
        if not fastq_format.search(os.path.basename(fastq_prefix)):
            continue
        kb_count_fastq_files_paths[
            os.path.basename(fastq_prefix)
        ] = kb_fastqs / os.path.basename(fastq_prefix)
        s3_kb_count_fastqs_bucket[
            os.path.basename(fastq_prefix)
        ] = s3_fastq_folder_bucket
        s3_kb_count_fastqs_prefix[os.path.basename(fastq_prefix)] = fastq_prefix
        fastq_t_config = TransferConfig(
            use_threads=False
        )  # testing purpose: comment this line if this runs into error.
        s3c.download_file(
            Bucket=s3_kb_count_fastqs_bucket[os.path.basename(fastq_prefix)],
            Key=s3_kb_count_fastqs_prefix[os.path.basename(fastq_prefix)],
            Filename=str(kb_count_fastq_files_paths[os.path.basename(fastq_prefix)]),
            Config=fastq_t_config,
        )

    # Build the command of running \`kb count\` to generate count matrices
    count_input_boolean = [
        "--keep-tmp",
        "--verbose",
        "--mm",
        "--tcc",
        "--overwrite",
        "--lamanno",
        "--nucleus",
        "--loom",
        "--h5ad",
    ]
    count_input_file_transfer_required = ["--tmp", "-o", "-w"]
    count_input_kb_indices = ["-i", "-g", "-c1", "-c2"]
    count_input_left_args = ["-t", "-m", "--workflow", "--filter", "-x"]

    kb_count_command = ["kb", "count"]
    for fastq_path in kb_count_fastq_files_paths.values():
        print(
            f"testing purpose - view the paths of individual fastqs on the EC2 instance, i.e. values of dictionary \`kb_count_fastq_files_paths\`: {kb_count_fastq_files_paths.values()}"
        )  # testing purpose
        kb_count_command += [str(fastq_path)]
    for input in count_input_boolean:
        if input in sys.argv:
            kb_count_command += [input]
    for input in count_input_file_transfer_required:
        if input in sys.argv:
            kb_count_command += [
                input,
                str(kb_count_paths[input[2:] + "_path"])
                if input == "--tmp"
                else str(kb_count_paths[input[1:] + "_path"]),
            ]
    for input in count_input_kb_indices:
        if input in sys.argv:
            kb_count_command += [input, str(kb_count_paths[input[1:] + "_path"])]
    for input in count_input_left_args:
        if input in sys.argv:
            kb_count_command += [
                input,
                getattr(args, input[2:])
                if input == ("--workflow" or "--filter")
                else getattr(args, input[1:]),
            ]
    print(
        f"testing purpose - view kb count command: {kb_count_command}"
    )  # testing purpose

    # Run the command to generate count matrices, and upload the output files on the EC2 instance back to AWS S3
    kb_count_failed = ut_log.log_command(
        logger,
        kb_count_command,
        shell=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
    )
    kb_count_t_config = TransferConfig(
        use_threads=False
    )  # testing purpose: comment this line if this runs into error. count matrices are not pretty big and don't necessarily need a transfer config
    if kb_count_failed:
        raise RuntimeError("kb data quantification failed")
    else:
        logger.info(f"Uploading kb quantification data from the EC2 instance to AWS S3")
        for root, dirs, files in os.walk(str(kb_count_outputs)):
            for file in files:
                s3c.upload_file(
                    Filename=os.path.join(root, file),
                    Bucket=s3_kb_count["s3_o_bucket"],
                    Key=posixpath.join(s3_kb_count["s3_o_prefix"], file),
                    Config=kb_count_t_config,
                )
                time.sleep(30)
                print(
                    f"testing purpose - see if kb data quantification outputs are uploaded to the correct s3 paths: "
                    + s3_kb_count["s3_o_bucket"]
                    + ", "
                    + posixpath.join(s3_kb_count["s3_o_prefix"], file)
                )  # testing purpose
    return


COMMAND_TO_FUNCTION = {
    "ref": parse_ref,
    "count": parse_count,
}


def setup_info_args(parser, parent):
    """Helper function to set up a subparser for the \`info\` command.
    
    Keyword Arguments:
    parser -- argparse parser to add the \`info\` command to (type: argparse.ArgumentParser)
    parent -- argparse parser parent of the newly added subcommand. used to inherit shared commands/flags (type: argparse.ArgumentParser)

    Return:
    the newly added parser (type: argparse.ArgumentParser)
    """
    parser_info = parser.add_parser(
        "info",
        description="Display kb-python package and citation information",
        help="Display kb-python package and citation information",
        parents=[parent],
        add_help=False,
    )
    return parser_info


def setup_ref_args(parser, parent):
    """Helper function to set up a subparser for the \`ref\` command.
    
    Keyword Arguments:
    parser -- argparse parser to add the \`ref\` command to (type: argparse.ArgumentParser)
    parent -- argparse parser parent of the newly added subcommand. used to inherit shared commands/flags (type: argparse.ArgumentParser)
    
    Return:
    the newly added parser (type: argparse.ArgumentParser)
    """
    workflow = (
        sys.argv[sys.argv.index("--workflow") + 1]
        if "--workflow" in sys.argv
        else "standard"
    )

    parser_ref = parser.add_parser(
        "ref",
        description="Build a kallisto index and transcript-to-gene mapping",
        help="Build a kallisto index and transcript-to-gene mapping",
        parents=[parent],
    )
    parser_ref._actions[0].help = parser_ref._actions[0].help.capitalize()

    # required arguments
    required_ref = parser_ref.add_argument_group("required arguments")
    required_ref.add_argument(
        "-i",
        metavar="INDEX",
        help="Path to the kallisto index to be constructed",
        type=str,
        required=True,
    )
    required_ref.add_argument(
        "-g",
        metavar="T2G",
        help="Path to transcript-to-gene mapping to be generated",
        type=str,
        required=True,
    )
    required_ref.add_argument(
        "-f1",
        metavar="FASTA",
        help=(
            "[Optional with -d] Path to the cDNA FASTA (lamanno, nucleus) "
            "or mismatch FASTA (kite) to be generated "
        ),
        type=str,
        required="-d" not in sys.argv,
    )

    # required arguments for lamanno and nucleus workflows
    required_lamanno = parser_ref.add_argument_group(
        "required arguments for \`lamanno\` and \`nucleus\` workflows"
    )
    required_lamanno.add_argument(
        "-f2",
        metavar="FASTA",
        help="Path to the intron FASTA to be generated",
        type=str,
        required=workflow in {"lamanno", "nucleus"}
        or any(arg in sys.argv for arg in {"--lamanno", "--nucleus"}),
    )
    required_lamanno.add_argument(
        "-c1",
        metavar="T2C",
        help="Path to generate cDNA transcripts-to-capture",
        type=str,
        required=workflow in {"lamanno", "nucleus"}
        or any(arg in sys.argv for arg in {"--lamanno", "--nucleus"}),
    )
    required_lamanno.add_argument(
        "-c2",
        metavar="T2C",
        help="Path to generate intron transcripts-to-capture",
        type=str,
        required=workflow in {"lamanno", "nucleus"}
        or any(arg in sys.argv for arg in {"--lamanno", "--nucleus"}),
    )

    # other optional and conditionally required arguments
    parser_ref.add_argument(
        "-d",
        help=(
            "Download a pre-built kallisto index (along with all necessary files) "
            "instead of building it locally"
        ),
        type=str,
        choices=["human", "mouse", "linnarsson"],
        required=False,
    )
    parser_ref.add_argument(
        "--lamanno",
        help="Deprecated. Use \`--workflow lamanno\` instead.",
        action="store_true",
    )
    parser_ref.add_argument(
        "--overwrite", help="Overwrite existing kallisto index", action="store_true"
    )
    parser_ref.add_argument(
        "fasta",
        help="Genomic FASTA file",
        type=str,
        nargs=None if "-d" not in sys.argv and workflow != "kite" else "?",
    )
    parser_ref.add_argument(
        "gtf",
        help="Reference GTF file",
        type=str,
        nargs=None if "-d" not in sys.argv and workflow != "kite" else "?",
    )
    parser_ref.add_argument(
        "feature",
        help=(
            "[\`kite\` workflow only] Path to TSV containing barcodes and feature names."
        ),
        type=str,
        nargs=None if "-d" not in sys.argv and workflow == "kite" else "?",
    )
    return parser_ref


def setup_count_args(parser, parent):
    """Helper function to set up a subparser for the \`count\` command.
    
    Keyword Arguments:
    parser -- argparse parser to add the \`count\` command to (type: argparse.ArgumentParser)
    parent -- argparse parser parent of the newly added subcommand. used to inherit shared commands/flags (type: argparse.ArgumentParser)
    
    Return:
    the newly added parser (type: argparse.ArgumentParser)
    """
    workflow = (
        sys.argv[sys.argv.index("--workflow") + 1]
        if "--workflow" in sys.argv
        else "standard"
    )

    # count
    parser_count = parser.add_parser(
        "count",
        description=(
            "Generate count matrices from a set of single-cell FASTQ files. "
            "Run \`--list\` to view single-cell technology information."
        ),  # noqa
        help="Generate count matrices from a set of single-cell FASTQ files",
        parents=[parent],
    )
    parser_count._actions[0].help = parser_count._actions[0].help.capitalize()

    # positional argument
    parser_count.add_argument(
        "fastq_folder", help="Path to the folder containing FASTQ files to be processed"
    )

    # required arguments
    required_count = parser_count.add_argument_group("required arguments")
    required_count.add_argument(
        "-i", metavar="INDEX", help="Path to kallisto index", type=str, required=True
    )
    required_count.add_argument(
        "-g",
        metavar="T2G",
        help="Path to transcript-to-gene mapping",
        type=str,
        required=True,
    )
    required_count.add_argument(
        "-x",
        metavar="TECHNOLOGY",
        help="Single-cell technology used (\`--list\` to view)",
        type=str,
        required=True,
    )

    # optional arguments
    parser_count.add_argument(
        "-o",
        metavar="OUT",
        help="Path to output directory (default: current directory)",
        type=str,
        default=".",
    )
    parser_count.add_argument(
        "-w",
        metavar="WHITELIST",
        help=(
            "Path to file of whitelisted barcodes to correct to. "
            "If not provided and bustools supports the technology, "
            "a pre-packaged whitelist is used. If not, the bustools "
            "whitelist command is used. (\`--list\` to view whitelists)"
        ),
        type=str,
    )
    parser_count.add_argument(
        "-t",
        metavar="THREADS",
        help="Number of threads to use (default: 8)",
        type=int,
        default=8,
    )
    parser_count.add_argument(
        "-m",
        metavar="MEMORY",
        help="Maximum memory used (default: 4G)",
        type=str,
        default="4G",
    )
    parser_count.add_argument(
        "--tcc",
        help="Generate a TCC matrix instead of a gene count matrix.",
        action="store_true",
    )
    parser_count.add_argument(
        "--overwrite", help="Overwrite existing output.bus file", action="store_true"
    )
    parser_count.add_argument(
        "--filter",
        help="Produce a filtered gene count matrix (default: bustools)",
        type=str,
        const="bustools",
        nargs="?",
        choices=["bustools"],
    )

    # required arguments for lamanno and nucleus workflows
    required_lamanno = parser_count.add_argument_group(
        "required arguments for \`lamanno\` and \`nucleus\` workflows"
    )
    required_lamanno.add_argument(
        "-c1",
        metavar="T2C",
        help="Path to cDNA transcripts-to-capture",
        type=str,
        required=workflow in {"lamanno", "nucleus"}
        or any(arg in sys.argv for arg in {"--lamanno", "--nucleus"}),
    )
    required_lamanno.add_argument(
        "-c2",
        metavar="T2C",
        help="Path to intron transcripts-to-captured",
        type=str,
        required=workflow in {"lamanno", "nucleus"}
        or any(arg in sys.argv for arg in {"--lamanno", "--nucleus"}),
    )

    # only one workflow (i.e. lamanno or nucleus) can be used in one run
    velocity_group = parser_count.add_mutually_exclusive_group()
    velocity_group.add_argument(
        "--lamanno",
        help="Deprecated. Use \`--workflow lamanno\` instead.",
        action="store_true",
    )
    velocity_group.add_argument(
        "--nucleus",
        help="Deprecated. Use \`--workflow nucleus\` instead.",
        action="store_true",
    )

    # loom and h5ad files can't be generated simultaneously in one run
    conversion_group = parser_count.add_mutually_exclusive_group()
    conversion_group.add_argument(
        "--loom", help="Generate loom file from count matrix", action="store_true"
    )
    conversion_group.add_argument(
        "--h5ad", help="Generate h5ad file from count matrix", action="store_true"
    )
    return parser_count


def get_parser():
    """ Construct and return the ArgumentParser object that parses input command.
    """

    # Main parser
    parser = argparse.ArgumentParser(
        prog="run_kb_python.py",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description="kb_python package",
    )
    parser._actions[0].help = parser._actions[0].help.capitalize()
    parser.add_argument(
        "--list",
        help="Display list of supported single-cell technologies",
        action="store_true",
    )
    parser.add_argument("--root_dir", default="/mnt")
    subparsers = parser.add_subparsers(dest="command", metavar="<CMD>",)

    # Add common options to this parent parser
    parent = argparse.ArgumentParser(add_help=False)
    parent.add_argument(
        "--workflow",
        help=(
            "Type of workflow. Use \`lamanno\` to calculate "
            "RNA velocity based on La Manno et al. 2018 logic. Use \`nucleus\` to "
            "calculate RNA velocity on single-nucleus RNA-seq reads (default: standard)"
        ),
        type=str,
        default="standard",
        choices=["standard", "lamanno", "nucleus", "kite"],
    )
    parent.add_argument(
        "--keep-tmp", help="Do not delete the tmp directory", action="store_true"
    )
    parent.add_argument(
        "--tmp_dir",
        help="AWS S3 bucket path to store temporary files generated in the run",
        type=str,
        required="--keep-tmp" in sys.argv,
    )
    parent.add_argument(
        "--verbose", help="Print debugging information", action="store_true"
    )

    # Command parsers
    setup_info_args(subparsers, argparse.ArgumentParser(add_help=False))
    parser_ref = setup_ref_args(subparsers, parent)
    parser_count = setup_count_args(subparsers, parent)

    command_to_parser = {
        "ref": parser_ref,
        "count": parser_count,
    }

    # Show help when no arguments are given
    if len(sys.argv) == 0:
        parser.print_help(sys.stderr)
        sys.exit(1)
    if len(sys.argv) == 1:
        if sys.argv[1] in command_to_parser:
            command_to_parser[sys.argv[1]].print_help(sys.stderr)
        else:
            parser.print_help(sys.stderr)
        sys.exit(1)

    return parser


def main(logger):
    """Command-line entrypoint. Download fastqs from the S3 bucket to an EC2 instance, build kallisto index, run kallisto peusoalignment, generate count matrices, and upload the data quantification results back to the S3 bucket.

    Keyword Argument:    
    logger -- Logger object that exposes the interface the code directly uses (type: logging.Logger)
    """
    # Parse input arguments
    parser = get_parser()
    args = parser.parse_args()
    print(f"testing purpose - what is sys.argv: {sys.argv}")
    print(f"testing purpose - what is args: {args}")

    # Root direcotry path on the EC2 instance
    root_dir = pathlib.Path(args.root_dir)

    if os.environ.get("AWS_BATCH_JOB_ID"):
        root_dir = root_dir / os.environ["AWS_BATCH_JOB_ID"]

    # Directory on the EC2 instance where relevant files for the run are stored
    run_dir = root_dir / "data"
    run_dir.mkdir(parents=True)

    # Run \`kb --info\` to see kb_python package details, or \`kb --list\` to see supported technologies list
    if "info" in sys.argv:
        display_info(logger)
    elif "--list" in sys.argv:
        display_technologies(logger)

    # Use the updated input format for non-standard workflows
    if any(arg in sys.argv for arg in {"--lamanno", "--nucleus"}):
        logger.warning(
            (
                "The \`--lamanno\` and \`--nucleus\` flags are deprecated. "
                "These options will be removed in a future release. "
                "Please use \`--workflow lamanno\` or \`--workflow nucleus\` instead when the --workflow argument is available."
            )
        )

    # Create tmp directory on the EC2 instance to store temporary files
    logger.debug("Creating tmp directory")
    tmp_dir = run_dir / "tmp"
    tmp_dir.mkdir(parents=True)

    # Run the command, and delete the temporary directory unless otherwise noted in the input
    try:
        logger.debug(args)
        print(
            f"testing purpose - see if args.command returns \`ref\` or \`count\`: {args.command}"
        )  # testing purpose
        COMMAND_TO_FUNCTION[args.command](args, run_dir, logger)
    finally:
        # Always clean temp dir
        if not args.keep_tmp:
            logger.debug("Removing tmp directory")
            shutil.rmtree(tmp_dir, ignore_errors=True)

    logger.info("Job completed")


if __name__ == "__main__":
    mainlogger, log_file, file_handler = ut_log.get_logger(__name__)
    s3c = boto3.client("s3")
    main(mainlogger)
VIASHMAIN
python "\$tempscript" &
wait "\$!"

VIASHEOF
